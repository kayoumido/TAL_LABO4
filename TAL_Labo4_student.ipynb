{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c7/HEIG-VD_Logo_96x29_RVB_ROUGE.png\" alt=\"HEIG-VD Logo\" width=\"250\"/>\n",
    "\n",
    "# Cours TAL - Laboratoire 4\n",
    "# Reconnaissance d'entités nommées (NER)\n",
    "\n",
    "**Objectifs**\n",
    "\n",
    "Appliquer l'outil de *Named Entity Recognition* fourni par NLTK sur le corpus Reuters en anglais, puis évaluer sa performance sur les données de test CoNLL 2003 possédant une annotation de référence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Expériences avec la NER de NLTK\n",
    "\n",
    "Le **but de cette partie** est d'utiliser le reconnaisseur d'entités nommées de NLTK pour extraire les entités nommées les plus fréquentes du corpus Reuters, avec leur type.\n",
    "* le NER de NLTK est tout simplement la fonction `nltk.ne_chunk`, qui s'applique sur un texte tokenisé, avec les POS tags -- la fonction est documentée dans le [livre NLTK, ch.7](http://www.nltk.org/book/ch07.html), section 5 (tout à la fin) ;\n",
    "*  le corpus Reuters contient environ 10'000 dépêches datant des années 1980, et il est fourni avec NLTK comme expliqué dans le [livre NLTK, ch.2](http://www.nltk.org/book/ch02.html), §1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/ducky/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "#nltk.downloader.Downloader().download('reuters') \n",
    "#nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "# à exécuter une seule fois pour télécharger les fichiers localement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En suivant les exemples fournis dans le livre NLTK, veuillez écrire le code qui permet de répondre aux questions suivantes, et écrire vos réponses dans une cellule *text markdown* ensuite.\n",
    "* Combien de fichiers (`fileids`) le corpus Reuters contient-il ?\n",
    "* Combien de phrases le corpus contient-il ?  (note : un seul appel de fonction est nécessaire)\n",
    "* Combien de mots le corpus contient-il ?  (note : un seul appel de fonction est nécessaire)\n",
    "* Veuillez afficher 5 *fileids* de votre choix (les noms, pas les contenus)\n",
    "* Pour un fichier (*fileid*) de votre choix, veuillez afficher son texte brut, puis la liste de ses phrases, puis enfin la liste de ses mots (avec la tokenization de référence du corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fileid \"test/14840\" raw text\n",
      "INDONESIAN COMMODITY EXCHANGE MAY EXPAND\n",
      "  The Indonesian Commodity Exchange is\n",
      "  likely to start trading in at least one new commodity, and\n",
      "  possibly two, during calendar 1987, exchange chairman Paian\n",
      "  Nainggolan said.\n",
      "      He told Reuters in a telephone interview that trading in\n",
      "  palm oil, sawn timber, pepper or tobacco was being considered.\n",
      "      Trading in either crude palm oil (CPO) or refined palm oil\n",
      "  may also be introduced. But he said the question was still\n",
      "  being considered by Trade Minister Rachmat Saleh and no\n",
      "  decision on when to go ahead had been made.\n",
      "      The fledgling exchange currently trades coffee and rubber\n",
      "  physicals on an open outcry system four days a week.\n",
      "      \"Several factors make us move cautiously,\" Nainggolan said.\n",
      "  \"We want to move slowly and safely so that we do not make a\n",
      "  mistake and undermine confidence in the exchange.\"\n",
      "      Physical rubber trading was launched in 1985, with coffee\n",
      "  added in January 1986. Rubber contracts are traded FOB, up to\n",
      "  five months forward. Robusta coffee grades four and five are\n",
      "  traded for prompt delivery and up to five months forward,\n",
      "  exchange officials said.\n",
      "      The trade ministry and exchange board are considering the\n",
      "  introduction of futures trading later for rubber, but one\n",
      "  official said a feasibility study was needed first. No\n",
      "  decisions are likely until after Indonesia's elections on April\n",
      "  23, traders said.\n",
      "      Trade Minister Saleh said on Monday that Indonesia, as the\n",
      "  world's second largest producer of natural rubber, should\n",
      "  expand its rubber marketing effort and he hoped development of\n",
      "  the exchange would help this.\n",
      "      Nainggolan said that the exchange was trying to boost\n",
      "  overseas interest by building up contacts with end-users.\n",
      "      He said teams had already been to South Korea and Taiwan to\n",
      "  encourage direct use of the exchange, while a delegation would\n",
      "  also visit Europe, Mexico and some Latin American states to\n",
      "  encourage participation.\n",
      "      Officials say the infant exchange has made a good start\n",
      "  although trading in coffee has been disappointing.\n",
      "      Transactions in rubber between the start of trading in\n",
      "  April 1985 and December 1986 totalled 9,595 tonnes, worth 6.9\n",
      "  mln dlrs FOB, plus 184.3 mln rupiah for rubber delivered\n",
      "  locally, the latest exchange report said.\n",
      "       Trading in coffee in calendar 1986 amounted to only 1,905\n",
      "  tonnes in 381 lots, valued at 6.87 billion rupiah.\n",
      "       Total membership of the exchange is now nine brokers and\n",
      "  44 traders.\n",
      "  \n",
      "\n",
      "\n",
      "Fileid \"test/14840\" list of sentences\n",
      "[['INDONESIAN', 'COMMODITY', 'EXCHANGE', 'MAY', 'EXPAND', 'The', 'Indonesian', 'Commodity', 'Exchange', 'is', 'likely', 'to', 'start', 'trading', 'in', 'at', 'least', 'one', 'new', 'commodity', ',', 'and', 'possibly', 'two', ',', 'during', 'calendar', '1987', ',', 'exchange', 'chairman', 'Paian', 'Nainggolan', 'said', '.'], ['He', 'told', 'Reuters', 'in', 'a', 'telephone', 'interview', 'that', 'trading', 'in', 'palm', 'oil', ',', 'sawn', 'timber', ',', 'pepper', 'or', 'tobacco', 'was', 'being', 'considered', '.'], ...]\n",
      "\n",
      "Fileid \"test/14840\" list of words\n",
      "['INDONESIAN', 'COMMODITY', 'EXCHANGE', 'MAY', ...]\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire ici le code nécessaire.\n",
    "# Q1\n",
    "#print(len(reuters.fileids()))\n",
    "# Q2\n",
    "#print(len(reuters.sents()))\n",
    "# Q3\n",
    "#print(len(reuters.words()))\n",
    "# Q4\n",
    "#reuters.fileids()[:5]\n",
    "\n",
    "# Q5\n",
    "fileid_name = 'test/14840'\n",
    "\n",
    "print('Fileid \"{}\" raw text'.format(fileid_name))\n",
    "print(reuters.raw(fileid_name))\n",
    "\n",
    "print('Fileid \"{}\" list of sentences'.format(fileid_name))\n",
    "print(reuters.sents(fileid_name))\n",
    "print()\n",
    "print('Fileid \"{}\" list of words'.format(fileid_name))\n",
    "print(reuters.words(fileid_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Combien de fichiers (fileids) le corpus Reuters contient-il ?  \n",
    "\n",
    "Il y a 10788 `fileids` dans le corpus.\n",
    "\n",
    "> Combien de phrases le corpus contient-il ?\n",
    "\n",
    "Il y a 54716 phrases dans le corpus.\n",
    "\n",
    "> Combien de mots le corpus contient-il ?\n",
    "\n",
    "Il y a 1720901 mots dans le corpus.\n",
    "\n",
    "> Veuillez afficher 5 fileids de votre choix (les noms, pas les contenus)\n",
    "\n",
    "['test/14826', 'test/14828', 'test/14829', 'test/14832', 'test/14833']\n",
    "\n",
    "> Pour un fichier (fileid) de votre choix, veuillez afficher son texte brut, puis la liste de ses phrases, puis enfin la liste de ses mots (avec la tokenization de référence du corpus)\n",
    "\n",
    "Voir code ci-dessus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vous demande maintenant d'expérimenter avec un seul texte du corpus Reuters, pour en extraire les entités nommées.   Veuillez répondre aux questions suivantes.\n",
    "\n",
    "* À partir du texte brut (*raw*) d'une dépêche de votre choix, effectuez la segmentation en phrases, et affichez le nombre de phrases\n",
    "* Sur une phrase de votre choix, effectuez avec NLTK la tokenization, le POS tagging et la NER, et affichez le résultat. (Si la phrase ne contient pas d'entité nommée (*chunk*), veuillez en choisir une autre.)\n",
    "* Quel est le type d'objet que retourne `ne_chunk` ?\n",
    "* Quelle est l'effet de l'attribut `binary` (True ou False) dans l'appel de `nltk.ne_chunk` ?\n",
    "* Veuillez rassembler les entités nommées de la phrase en une seule liste, de la forme `[('MTBE', 'ORGANIZATION'), ('United States', 'GPE')]` (veuillez notamment joindre en une seule chaîne les mots des entités à plusieurs mots).\n",
    "\n",
    "Pour votre information, le `ne_chunk()` de NLTK annote les types suivants d'entités nommées : ORGANIZATION, PERSON, LOCATION, DATE, TIME, MONEY, PERCENT, FACILITY, GPE (= *geo-political entity*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici le code nécessaire.\n",
    "def tokenize_text_into_sents(text):\n",
    "    return nltk.tokenize.sent_tokenize(text)\n",
    "\n",
    "def tokenize_sent_into_words(sent):\n",
    "    return nltk.tokenize.word_tokenize(sent)\n",
    "\n",
    "def get_named_entities(chunks):\n",
    "    lchunks = []\n",
    "    for chunk in chunks:\n",
    "        # check if the current chunk is a named entity\n",
    "        if hasattr(chunk, 'label'):\n",
    "            word = ' '.join([c[0] for c in chunk])\n",
    "            lchunks.append((word, chunk.label()))\n",
    "\n",
    "    return lchunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('``', '``')\n",
      "('We', 'PRP')\n",
      "('want', 'VBP')\n",
      "('to', 'TO')\n",
      "('move', 'VB')\n",
      "('slowly', 'RB')\n",
      "('and', 'CC')\n",
      "('safely', 'RB')\n",
      "('so', 'IN')\n",
      "('that', 'IN')\n",
      "('we', 'PRP')\n",
      "('do', 'VBP')\n",
      "('not', 'RB')\n",
      "('make', 'VB')\n",
      "('a', 'DT')\n",
      "('mistake', 'NN')\n",
      "('and', 'CC')\n",
      "('undermine', 'JJ')\n",
      "('confidence', 'NN')\n",
      "('in', 'IN')\n",
      "('the', 'DT')\n",
      "('exchange', 'NN')\n",
      "('.', '.')\n",
      "(\"''\", \"''\")\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "fileid = reuters.fileids()[6]\n",
    "raw_text = reuters.raw(fileid)\n",
    "sent = split_raw_text_into_phrases(raw_text)[0]\n",
    "words = tokenize_sent_into_words(sent)\n",
    "tagged_words = nltk.pos_tag(words)\n",
    "chunks = nltk.ne_chunk(tagged_words)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(chunk)\n",
    "        \n",
    "print(get_named_entities(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary = True\n",
      "(S\n",
      "  (GPE THAI/NNP)\n",
      "  (ORGANIZATION TRADE/NNP)\n",
      "  DEFICIT/NNP\n",
      "  WIDENS/NNP\n",
      "  IN/NNP\n",
      "  FIRST/NNP\n",
      "  QUARTER/NNP\n",
      "  (GPE Thailand/NNP)\n",
      "  's/POS\n",
      "  trade/NN\n",
      "  deficit/NN\n",
      "  widened/VBD\n",
      "  to/TO\n",
      "  4.5/CD\n",
      "  billion/CD\n",
      "  baht/NNS\n",
      "  in/IN\n",
      "  the/DT\n",
      "  first/JJ\n",
      "  quarter/NN\n",
      "  of/IN\n",
      "  1987/CD\n",
      "  from/IN\n",
      "  2.1/CD\n",
      "  billion/CD\n",
      "  a/DT\n",
      "  year/NN\n",
      "  ago/RB\n",
      "  ,/,\n",
      "  the/DT\n",
      "  (ORGANIZATION Business/NNP Economics/NNP Department/NNP)\n",
      "  said/VBD\n",
      "  ./.)\n",
      "\n",
      "binary = False\n",
      "(S\n",
      "  (NE THAI/NNP)\n",
      "  TRADE/NNP\n",
      "  DEFICIT/NNP\n",
      "  WIDENS/NNP\n",
      "  IN/NNP\n",
      "  FIRST/NNP\n",
      "  QUARTER/NNP\n",
      "  Thailand/NNP\n",
      "  's/POS\n",
      "  trade/NN\n",
      "  deficit/NN\n",
      "  widened/VBD\n",
      "  to/TO\n",
      "  4.5/CD\n",
      "  billion/CD\n",
      "  baht/NNS\n",
      "  in/IN\n",
      "  the/DT\n",
      "  first/JJ\n",
      "  quarter/NN\n",
      "  of/IN\n",
      "  1987/CD\n",
      "  from/IN\n",
      "  2.1/CD\n",
      "  billion/CD\n",
      "  a/DT\n",
      "  year/NN\n",
      "  ago/RB\n",
      "  ,/,\n",
      "  the/DT\n",
      "  (NE Business/NNP Economics/NNP Department/NNP)\n",
      "  said/VBD\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(\"binary = True\")\n",
    "print(nltk.ne_chunk(tagged_words, binary=False))\n",
    "print()\n",
    "print(\"binary = False\")\n",
    "print(nltk.ne_chunk(tagged_words, binary=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Quel est le type d'objet que retourne ne_chunk ?\n",
    "\n",
    "Il retourne un objet `<class 'nltk.tree.Tree'>`.\n",
    "\n",
    "> Quelle est l'effet de l'attribut binary (True ou False) dans l'appel de nltk.ne_chunk ?\n",
    "\n",
    "Cet attribut va determiner quel `treebank POS tagger` la fonction va utiliser. **True** = `english_ace_binary.pickle`, **False** = `english_ace_multiclass.pickle`.\n",
    "\n",
    "En comparant l'output de la fonction en changement la valeur de cet attribut, on peu voir que quand il est set a **True** la fonction ne va pas utilisé les types d'entités nommées mentionnez dans la donnée lors de l'annotation.\n",
    "\n",
    "E.g.\n",
    "\n",
    "|       | binary = True            | binary = False |\n",
    "| ----- | :----------------------- | -------------- |\n",
    "| THAI  | (GPE THAI/NNP)           | (NE THAI/NNP)  |\n",
    "| TRADE | (ORGANIZATION TRADE/NNP) | TRADE/NNP      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Veuillez écrire une fonction, commençant par la ligne `def extract_named_entities(text):` qui retourne la liste des entités nommées avec leur type, à partir d'un texte donné (string).  Inspirez-vous de votre réponse à la dernière question ci-dessus.\n",
    "* Testez votre fonction sur le texte que vous avez utilisé ci-dessus.  \n",
    "* Observez vous des erreurs de détection (rappel ou précision) et/ou d'étiquetage ?  Merci d'en indiquer quelques-unes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici le code nécessaire.\n",
    "def extract_named_entities(text):\n",
    "    sent = split_raw_text_into_phrases(raw_text)\n",
    "    tagged_words = []    \n",
    "    for sent in split_raw_text_into_phrases(text):\n",
    "        words = tokenize_sent_into_words(sent)\n",
    "        tagged_words = tagged_words + nltk.pos_tag(words)\n",
    "\n",
    "    chunks = nltk.ne_chunk(tagged_words)\n",
    "    return get_named_entities(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('INDONESIAN', 'GPE'),\n",
       " ('COMMODITY', 'ORGANIZATION'),\n",
       " ('Indonesian', 'GPE'),\n",
       " ('Paian Nainggolan', 'PERSON'),\n",
       " ('Reuters', 'ORGANIZATION'),\n",
       " ('CPO', 'ORGANIZATION'),\n",
       " ('Trade', 'PERSON'),\n",
       " ('Rachmat Saleh', 'PERSON'),\n",
       " ('Nainggolan', 'PERSON'),\n",
       " ('FOB', 'ORGANIZATION'),\n",
       " ('Robusta', 'PERSON'),\n",
       " ('Indonesia', 'GPE'),\n",
       " ('Saleh', 'PERSON'),\n",
       " ('Indonesia', 'GPE'),\n",
       " ('Nainggolan', 'PERSON'),\n",
       " ('South Korea', 'GPE'),\n",
       " ('Taiwan', 'GPE'),\n",
       " ('Europe', 'GPE'),\n",
       " ('Mexico', 'GPE'),\n",
       " ('American', 'GPE'),\n",
       " ('FOB', 'ORGANIZATION'),\n",
       " ('Total', 'ORGANIZATION')]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Veuillez écrire ici le code nécessaire.\n",
    "extract_named_entities(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Erreurs observées.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veuillez parcourir tous les textes du corpus Reuters et collecter toutes les entités nommées dans une liste.  Créez ensuite une `FreqDist` et affichez les 30 NE les plus fréquentes avec leur nombre d'occurrences.  Combien de temps approximativement prend cette opération ?  (Suggestion : augmentez progressivement le nombre de fileids que vous traitez, pour estimer le temps total.)  Veuillez commenter le résultat obtenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici le code nécessaire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici les commentaires sur le résultat.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quel est le nombre total de NE trouvées (occurrences pas nécessairement différentes) et quel est le nombre de NE différentes ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici le code nécessaire pour répondre aux deux questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Évaluer la fonction de NER de NLTK sur les données CoNLL 2003\n",
    "\n",
    "À la conférence [CoNLL](https://www.clips.uantwerpen.be/pages/past-workshops) 2003, une des tâches compétitives consistait à tester des systèmes de NER sur l'anglais (voir [la description de la tâche et les scores obtenus](https://www.clips.uantwerpen.be/conll2003/ner/)).  Les ressources annotées ne sont pas disponibles via CoNLL, mais on peut en trouver [une copie sur le web](https://sourceforge.net/p/text-analysis/svn/1243/tree/text-analysis/trunk/Corpora/CoNLL/2003/) (une [autre copie](https://github.com/synalp/NER/tree/master/corpus/CoNLL-2003) est aussi disponible).  Les textes proviennent du [corpus Reuters](http://trec.nist.gov/data/reuters/reuters.html).  Pour mémoire, une autre source de données est le [corpus WikiNER](https://github.com/dice-group/FOX/tree/master/input/Wikiner).\n",
    "\n",
    "Le format d'annotation comprend 4 colonnes séparées par un espace.  Ce format ressemble au format \"conll\" que nous avons utilisé pour le *POS tagging* et le *parsing*.  Chaque ligne correspond à un mot, et une ligne vide sépare les phrases.  Sur chaque ligne, le 1er item est le mot, le 2e est le POS tag, le 3e est un tag qui indique le groupe syntaxique, et le 4e le tag qui indique l'entité nommée.  (À vous d'étudier ce tag plus en détail.)  Il y a des données d'entraînement (`eng.train`), et trois fichiers de test (`eng.testa`, `eng.testb` et `eng.testc`, le 2e ayant servi pour l'évaluation finale).\n",
    "\n",
    "**Travail demandé**\n",
    "\n",
    "Les questions qui suivent (inspirées des étapes de ce [tutoriel en ligne](https://pythonprogramming.net/testing-stanford-ner-taggers-for-accuracy/)) vous permettront d'estimer la \"justesse\" du NER de NLTK  sur les données CoNLL (seulement l'*accuracy*, pas le rappel et la précision)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Le premier objectif** est d'importer les données CoNLL 2003 dans ce notebook et adapter leur format pour qu'il soit comparable à celui de `nltk.ne_chunk()`, ce qui nécessite aussi la modification de l'output de cette fonction.\n",
    "\n",
    "En examinant les fichiers `eng.testa`, `eng.testb` et `eng.testc`, décrivez brièvement le format d'annotation CoNLL (2-3 phrases) et indiquez les types de NER annotés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici votre réponse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veuillez écrire une fonction qui ouvre un fichier CoNLL (p.ex. `eng.testa`) et crée deux listes :\n",
    "1. la liste des paires (token, pos_tag) que vous passerez plus tard à `ne_chunk()`;\n",
    "2. la liste des paires (token, ner_tag) où ner_tag est l'une des catégories de NER que vous avez indiquées plus haut.\n",
    "Ces listes seront stockées comme ci-dessous.  Appelez ensuite cette fonction sur les trois fichiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voici comment stocker les données dans une structure:\n",
    "filenames = ['eng.testa', 'eng.testb', 'eng.testc']\n",
    "test_data = dict()\n",
    "for f in filenames:\n",
    "    test_data[f] = dict()\n",
    "    test_data[f]['words'] = []\n",
    "    test_data[f]['keytags'] = [] # 'key' signifie correct\n",
    "    test_data[f]['reptags'] = [] # 'rep' pour réponse du système    \n",
    "# def conll2nltk(filename='eng.testa'):\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici le code de la fonction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez appliquer la fonction aux trois noms de fichier fournis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Le second objectif** est de donner les mots à `ne_chunk()`, obtenir le résultat de la NER, et le stocker dans la même forme que l'annotation de référence (des paires de (token, TAG) pour tous les tokens).  Ces résultats seront ajoutés à la structure `test_data` dans le champ 'reptags' (pour \"response tags\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici le code pour les 3 datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veuillez vérifier, pour chaque fichier, que les listes 'keytags' et 'reptags' ont le même nombre de mots, en les affichant côte à côte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici le code pour les 3 datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Le 3e et dernier objectif** est de calculer la pourcentage d'étiquettes correctes dans les trois fichiers par rapport au nombre de mots de chacun.  \n",
    "\n",
    "Pour ce faire, remarquez que les étiquettes assignées par NLTK sont ORGANIZATION, PERSON, LOCATION, DATE, TIME, MONEY, PERCENT, FACILITY, GPE (= geo-political entity), alors que celles des fichiers CoNLL sont celles que vous avez indiquées en réponse plus haut.  Pour comptabiliser les réponses correctes, il faut d'abord définir une fonction appelée `compatible(n, c)` qui indique si deux étiquettes (l'une de NLTK, l'autre de CoNLL) sont conceptuellement identiques (c'est-à-dire qu'elles ont la même signification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez définir ici une fonction de comparaison des tags NER.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez définir ici une fonction qui compare deux listes de (mot, tag) et qui\n",
    "# retourne le pourcentage de (mot, tag) identiques selon la fonction \"compatible()\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez appliquer ici la fonction ci-dessus et afficher les scores sur les 3 datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note sur `nltk.ne_chunk()`.** Pour mémoire, signalons que le résultat de `nltk.ne_chunk()`, qui est un arbre, peut être transformé en une chaîne de caractères multi-lignes formatée selon les guidelines CoNLL grâce à la méthode `nltk.chunk.tree2conllstr()`.  Par ailleurs, pour comparer deux étiquetages de mots (listes de paires (mot, tag)), on peut utiliser directement la fonction `nltk.metrics.scores.accuracy` de NLTK, pour autant que les tags soient comparables avec `==`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarque finale\n",
    "Il est possible d'appliquer de la même façon [l'outil de NER avec CRF fourni par Stanford](https://nlp.stanford.edu/software/CRF-NER.html).  Les CRF, *Conditional Random Fields*, sont le modèle probabiliste qui est utilisé dans cet outil.\n",
    "\n",
    "Comme dans le cas du *POS tagger* CoreNLP de Stanford, on peut invoquer l'outil en ligne de commande (suivant les exemples fournis [ici](https://nlp.stanford.edu/software/CRF-NER.html#Starting) ou [ici](https://nlp.stanford.edu/software/crf-faq.shtml), notamment pour les [options de sortie](https://nlp.stanford.edu/software/crf-faq.shtml#j)).  Ou alors, on peut utiliser le [wrapper `StanfordNERTagger` de NLTK](http://www.nltk.org/api/nltk.tag.html?#nltk.tag.stanford.StanfordNERTagger).  On peut alors voir que les performances sont plus élevées que celles de `nltk.ne_chunk()`. \n",
    "\n",
    "Une [version plus élaborée de NER est fournie par Stanford dans le cadre de la boîte à outils CoreNLP](https://stanfordnlp.github.io/CoreNLP/ner.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin du laboratoire 4\n",
    "\n",
    "Merci de nettoyer votre feuille, exécuter une dernière fois toutes les instructions, sauvegarder le résultat, et le soumettre sur Cyberlearn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
