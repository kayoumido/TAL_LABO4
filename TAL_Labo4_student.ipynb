{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c7/HEIG-VD_Logo_96x29_RVB_ROUGE.png\" alt=\"HEIG-VD Logo\" width=\"250\"/>\n",
    "\n",
    "# Cours TAL - Laboratoire 4\n",
    "# Reconnaissance d'entités nommées (NER)\n",
    "\n",
    "**Objectifs**\n",
    "\n",
    "Appliquer l'outil de *Named Entity Recognition* fourni par NLTK sur le corpus Reuters en anglais, puis évaluer sa performance sur les données de test CoNLL 2003 possédant une annotation de référence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Expériences avec la NER de NLTK\n",
    "\n",
    "Le **but de cette partie** est d'utiliser le reconnaisseur d'entités nommées de NLTK pour extraire les entités nommées les plus fréquentes du corpus Reuters, avec leur type.\n",
    "* le NER de NLTK est tout simplement la fonction `nltk.ne_chunk`, qui s'applique sur un texte tokenisé, avec les POS tags -- la fonction est documentée dans le [livre NLTK, ch.7](http://www.nltk.org/book/ch07.html), section 5 (tout à la fin) ;\n",
    "*  le corpus Reuters contient environ 10'000 dépêches datant des années 1980, et il est fourni avec NLTK comme expliqué dans le [livre NLTK, ch.2](http://www.nltk.org/book/ch02.html), §1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "# nltk.downloader.Downloader().download('reuters') \n",
    "# à exécuter une seule fois pour télécharger les fichiers localement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En suivant les exemples fournis dans le livre NLTK, veuillez écrire le code qui permet de répondre aux questions suivantes, et écrire vos réponses dans une cellule *text markdown* ensuite.\n",
    "* Combien de fichiers (`fileids`) le corpus Reuters contient-il ?\n",
    "* Combien de phrases le corpus contient-il ?  (note : un seul appel de fonction est nécessaire)\n",
    "* Combien de mots le corpus contient-il ?  (note : un seul appel de fonction est nécessaire)\n",
    "* Veuillez afficher 5 *fileids* de votre choix (les noms, pas les contenus)\n",
    "* Pour un fichier (*fileid*) de votre choix, veuillez afficher son texte brut, puis la liste de ses phrases, puis enfin la liste de ses mots (avec la tokenization de référence du corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici le code nécessaire.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vous demande maintenant d'expérimenter avec un seul texte du corpus Reuters, pour en extraire les entités nommées.   Veuillez répondre aux questions suivantes.\n",
    "\n",
    "* À partir du texte brut (*raw*) d'une dépêche de votre choix, effectuez la segmentation en phrases, et affichez le nombre de phrases\n",
    "* Sur une phrase de votre choix, effectuez avec NLTK la tokenization, le POS tagging et la NER, et affichez le résultat. (Si la phrase ne contient pas d'entité nommée (*chunk*), veuillez en choisir une autre.)\n",
    "* Quel est le type d'objet que retourne `ne_chunk` ?\n",
    "* Quelle est l'effet de l'attribut `binary` (True ou False) dans l'appel de `nltk.ne_chunk` ?\n",
    "* Veuillez rassembler les entités nommées de la phrase en une seule liste, de la forme `[('MTBE', 'ORGANIZATION'), ('United States', 'GPE')]` (veuillez notamment joindre en une seule chaîne les mots des entités à plusieurs mots).\n",
    "\n",
    "Pour votre information, le `ne_chunk()` de NLTK annote les types suivants d'entités nommées : ORGANIZATION, PERSON, LOCATION, DATE, TIME, MONEY, PERCENT, FACILITY, GPE (= *geo-political entity*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici le code nécessaire.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Veuillez écrire une fonction, commençant par la ligne `def extract_named_entities(text):` qui retourne la liste des entités nommées avec leur type, à partir d'un texte donné (string).  Inspirez-vous de votre réponse à la dernière question ci-dessus.\n",
    "* Testez votre fonction sur le texte que vous avez utilisé ci-dessus.  \n",
    "* Observez vous des erreurs de détection (rappel ou précision) et/ou d'étiquetage ?  Merci d'en indiquer quelques-unes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici le code nécessaire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici le code nécessaire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici le commentaire sur les erreurs observées.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veuillez parcourir tous les textes du corpus Reuters et collecter toutes les entités nommées dans une liste.  Créez ensuite une `FreqDist` et affichez les 30 NE les plus fréquentes avec leur nombre d'occurrences.  Combien de temps approximativement prend cette opération ?  (Suggestion : augmentez progressivement le nombre de fileids que vous traitez, pour estimer le temps total.)  Veuillez commenter le résultat obtenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici le code nécessaire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici les commentaires sur le résultat.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quel est le nombre total de NE trouvées (occurrences pas nécessairement différentes) et quel est le nombre de NE différentes ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici le code nécessaire pour répondre aux deux questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Évaluer la fonction de NER de NLTK sur les données CoNLL 2003\n",
    "\n",
    "À la conférence [CoNLL](https://www.clips.uantwerpen.be/pages/past-workshops) 2003, une des tâches compétitives consistait à tester des systèmes de NER sur l'anglais (voir [la description de la tâche et les scores obtenus](https://www.clips.uantwerpen.be/conll2003/ner/)).  Les ressources annotées ne sont pas disponibles via CoNLL, mais on peut en trouver [une copie sur le web](https://sourceforge.net/p/text-analysis/svn/1243/tree/text-analysis/trunk/Corpora/CoNLL/2003/) (une [autre copie](https://github.com/synalp/NER/tree/master/corpus/CoNLL-2003) est aussi disponible).  Les textes proviennent du [corpus Reuters](http://trec.nist.gov/data/reuters/reuters.html).  Pour mémoire, une autre source de données est le [corpus WikiNER](https://github.com/dice-group/FOX/tree/master/input/Wikiner).\n",
    "\n",
    "Le format d'annotation comprend 4 colonnes séparées par un espace.  Ce format ressemble au format \"conll\" que nous avons utilisé pour le *POS tagging* et le *parsing*.  Chaque ligne correspond à un mot, et une ligne vide sépare les phrases.  Sur chaque ligne, le 1er item est le mot, le 2e est le POS tag, le 3e est un tag qui indique le groupe syntaxique, et le 4e le tag qui indique l'entité nommée.  (À vous d'étudier ce tag plus en détail.)  Il y a des données d'entraînement (`eng.train`), et trois fichiers de test (`eng.testa`, `eng.testb` et `eng.testc`, le 2e ayant servi pour l'évaluation finale).\n",
    "\n",
    "**Travail demandé**\n",
    "\n",
    "Les questions qui suivent (inspirées des étapes de ce [tutoriel en ligne](https://pythonprogramming.net/testing-stanford-ner-taggers-for-accuracy/)) vous permettront d'estimer la \"justesse\" du NER de NLTK  sur les données CoNLL (seulement l'*accuracy*, pas le rappel et la précision)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Le premier objectif** est d'importer les données CoNLL 2003 dans ce notebook et adapter leur format pour qu'il soit comparable à celui de `nltk.ne_chunk()`, ce qui nécessite aussi la modification de l'output de cette fonction.\n",
    "\n",
    "En examinant les fichiers `eng.testa`, `eng.testb` et `eng.testc`, décrivez brièvement le format d'annotation CoNLL (2-3 phrases) et indiquez les types de NER annotés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici votre réponse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veuillez écrire une fonction qui ouvre un fichier CoNLL (p.ex. `eng.testa`) et crée deux listes :\n",
    "1. la liste des paires (token, pos_tag) que vous passerez plus tard à `ne_chunk()`;\n",
    "2. la liste des paires (token, ner_tag) où ner_tag est l'une des catégories de NER que vous avez indiquées plus haut.\n",
    "Ces listes seront stockées comme ci-dessous.  Appelez ensuite cette fonction sur les trois fichiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voici comment stocker les données dans une structure:\n",
    "filenames = ['eng.testa', 'eng.testb', 'eng.testc']\n",
    "test_data = dict()\n",
    "for f in filenames:\n",
    "    test_data[f] = dict()\n",
    "    test_data[f]['words'] = []\n",
    "    test_data[f]['keytags'] = [] # 'key' signifie correct\n",
    "    test_data[f]['reptags'] = [] # 'rep' pour réponse du système    \n",
    "# def conll2nltk(filename='eng.testa'):\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici le code de la fonction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez appliquer la fonction aux trois noms de fichier fournis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Le second objectif** est de donner les mots à `ne_chunk()`, obtenir le résultat de la NER, et le stocker dans la même forme que l'annotation de référence (des paires de (token, TAG) pour tous les tokens).  Ces résultats seront ajoutés à la structure `test_data` dans le champ 'reptags' (pour \"response tags\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici le code pour les 3 datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veuillez vérifier, pour chaque fichier, que les listes 'keytags' et 'reptags' ont le même nombre de mots, en les affichant côte à côte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici le code pour les 3 datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Le 3e et dernier objectif** est de calculer la pourcentage d'étiquettes correctes dans les trois fichiers par rapport au nombre de mots de chacun.  \n",
    "\n",
    "Pour ce faire, remarquez que les étiquettes assignées par NLTK sont ORGANIZATION, PERSON, LOCATION, DATE, TIME, MONEY, PERCENT, FACILITY, GPE (= geo-political entity), alors que celles des fichiers CoNLL sont celles que vous avez indiquées en réponse plus haut.  Pour comptabiliser les réponses correctes, il faut d'abord définir une fonction appelée `compatible(n, c)` qui indique si deux étiquettes (l'une de NLTK, l'autre de CoNLL) sont conceptuellement identiques (c'est-à-dire qu'elles ont la même signification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez définir ici une fonction de comparaison des tags NER.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez définir ici une fonction qui compare deux listes de (mot, tag) et qui\n",
    "# retourne le pourcentage de (mot, tag) identiques selon la fonction \"compatible()\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez appliquer ici la fonction ci-dessus et afficher les scores sur les 3 datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note sur `nltk.ne_chunk()`.** Pour mémoire, signalons que le résultat de `nltk.ne_chunk()`, qui est un arbre, peut être transformé en une chaîne de caractères multi-lignes formatée selon les guidelines CoNLL grâce à la méthode `nltk.chunk.tree2conllstr()`.  Par ailleurs, pour comparer deux étiquetages de mots (listes de paires (mot, tag)), on peut utiliser directement la fonction `nltk.metrics.scores.accuracy` de NLTK, pour autant que les tags soient comparables avec `==`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarque finale\n",
    "Il est possible d'appliquer de la même façon [l'outil de NER avec CRF fourni par Stanford](https://nlp.stanford.edu/software/CRF-NER.html).  Les CRF, *Conditional Random Fields*, sont le modèle probabiliste qui est utilisé dans cet outil.\n",
    "\n",
    "Comme dans le cas du *POS tagger* CoreNLP de Stanford, on peut invoquer l'outil en ligne de commande (suivant les exemples fournis [ici](https://nlp.stanford.edu/software/CRF-NER.html#Starting) ou [ici](https://nlp.stanford.edu/software/crf-faq.shtml), notamment pour les [options de sortie](https://nlp.stanford.edu/software/crf-faq.shtml#j)).  Ou alors, on peut utiliser le [wrapper `StanfordNERTagger` de NLTK](http://www.nltk.org/api/nltk.tag.html?#nltk.tag.stanford.StanfordNERTagger).  On peut alors voir que les performances sont plus élevées que celles de `nltk.ne_chunk()`. \n",
    "\n",
    "Une [version plus élaborée de NER est fournie par Stanford dans le cadre de la boîte à outils CoreNLP](https://stanfordnlp.github.io/CoreNLP/ner.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin du laboratoire 4\n",
    "\n",
    "Merci de nettoyer votre feuille, exécuter une dernière fois toutes les instructions, sauvegarder le résultat, et le soumettre sur Cyberlearn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
